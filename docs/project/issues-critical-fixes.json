{
  "critical_fixes": [
    {
      "title": "Fix ES Module configuration and resolve module warnings",
      "body": "## ðŸš¨ Critical Issue: Module Type Configuration\n\n### Problem Description\nThe application is currently generating performance-impacting warnings due to incorrect ES Module configuration:\n```\n[MODULE_TYPELESS_PACKAGE_JSON] Warning: Module type of file:///home/ec2-user/workspace/lightningtalk-circle/server/app.js is not specified and it doesn't parse as CommonJS.\nReparsing as ES module because module syntax was detected. This incurs a performance overhead.\n```\n\n### Root Cause Analysis\n- `package.json` missing `\"type\": \"module\"` declaration\n- ES Module syntax used throughout codebase but not properly configured\n- Node.js attempting CommonJS parsing first, then falling back to ES Module\n\n### Required Changes\n\n#### 1. Package.json Configuration\n```json\n{\n  \"type\": \"module\",\n  \"exports\": {\n    \".\": \"./server/app.js\"\n  }\n}\n```\n\n#### 2. Import Path Updates\nUpdate any relative imports missing file extensions:\n```javascript\n// Before\nimport { DatabaseService } from './services/database';\n// After\nimport { DatabaseService } from './services/database.js';\n```\n\n#### 3. Jest Configuration Update\nUpdate `jest.config.js` for ES Module support:\n```javascript\nexport default {\n  preset: 'node',\n  extensionsToTreatAsEsm: ['.js'],\n  globals: {\n    'ts-jest': {\n      useESM: true\n    }\n  },\n  moduleNameMapping: {\n    '^(\\\\./.*)\\\\.js$': '$1'\n  }\n};\n```\n\n### Acceptance Criteria\n- [ ] No module type warnings during server startup\n- [ ] All existing functionality works without breaking changes\n- [ ] Tests pass with updated configuration\n- [ ] Performance overhead eliminated\n- [ ] CI/CD pipeline runs successfully\n\n### Testing Strategy\n- [ ] Verify server starts without warnings\n- [ ] Run full test suite to ensure no regressions\n- [ ] Test build process in CI environment\n- [ ] Validate all import statements resolve correctly\n\n### Implementation Priority\nðŸ”¥ **CRITICAL** - Must be completed immediately as it affects:\n- Development experience\n- Production performance\n- Build reliability\n\n### Estimated Effort\n**1-2 hours** - Straightforward configuration changes with thorough testing\n\n### Dependencies\nNone - can be implemented immediately\n\n### Notes for Copilot\n- Focus on maintaining backward compatibility\n- Ensure all file extensions are explicit in imports\n- Test changes incrementally to avoid breaking the build\n- Update any CI/CD configurations that might be affected",
      "labels": ["type:bug", "priority:critical", "component:infrastructure", "effort:small", "status:ready"]
    },
    {
      "title": "Fix failing unit tests and improve test reliability",
      "body": "## ðŸ§ª Critical Issue: Failing Unit Tests\n\n### Problem Description\nMultiple unit tests are currently failing, preventing reliable CI/CD and development workflow:\n\n#### Test Failures Identified:\n1. **DataManager appendData test**: Object merging logic incorrect\n2. **Event validation test**: Date validation failing for future dates\n\n### Detailed Analysis\n\n#### Issue 1: Object Merging Logic\n```javascript\n// Current failing implementation in tests/unit/server-utils.test.js:192\nexpect(result.settings).toEqual({ theme: 'dark', language: 'ja' });\n// Actual result: { \"language\": \"ja\" }\n// Expected: { \"theme\": \"dark\", \"language\": \"ja\" }\n```\n\n**Root Cause**: The `appendData` method's object merging is overwriting instead of merging.\n\n#### Issue 2: Date Validation Logic\n```javascript\n// Error: \"Event date must be in the future\"\n// Test data using hardcoded future date that may now be in the past\nconst validEventData = {\n  date: '2025-01-15T19:00:00Z'  // This date might be too close or in the past\n};\n```\n\n### Required Fixes\n\n#### 1. Fix DataManager.appendData Method\n```javascript\nasync appendData(filename, newData) {\n  const existingData = await this.loadData(filename) || {};\n  \n  if (Array.isArray(existingData)) {\n    return await this.saveData(filename, [...existingData, newData]);\n  } else {\n    // Fix: Properly merge objects instead of overwriting\n    const updatedData = { ...existingData, ...newData };\n    return await this.saveData(filename, updatedData);\n  }\n}\n```\n\n#### 2. Fix Event Date Validation Test\n```javascript\n// Use dynamic future date generation\nconst validEventData = {\n  title: 'Tech Lightning Talk',\n  date: new Date(Date.now() + 24 * 60 * 60 * 1000).toISOString(), // Tomorrow\n  location: 'Tokyo Conference Room',\n  description: 'Monthly tech sharing event'\n};\n```\n\n#### 3. Test Coverage Improvement\nAddress 0% coverage by ensuring tests actually import and test the real implementation:\n```javascript\n// Import actual implementation instead of defining inline\nimport { DataManager, validateEventData } from '../../server/services/utils.js';\n```\n\n### Acceptance Criteria\n- [ ] All unit tests pass consistently\n- [ ] DataManager object merging works correctly\n- [ ] Event date validation accepts valid future dates\n- [ ] Test coverage increases from 0% to at least 50%\n- [ ] Tests run reliably in CI environment\n- [ ] No flaky test behavior due to timing issues\n\n### Implementation Steps\n\n#### Phase 1: Fix Immediate Failures\n1. [ ] Create/update actual implementation files for tested functions\n2. [ ] Fix object merging logic in DataManager\n3. [ ] Update date validation test with dynamic dates\n4. [ ] Verify tests pass locally\n\n#### Phase 2: Improve Test Quality\n1. [ ] Add edge case tests for DataManager\n2. [ ] Improve test data management\n3. [ ] Add proper setup/teardown for database tests\n4. [ ] Verify coverage reporting works\n\n### Testing Strategy\n- [ ] Run tests individually to isolate issues\n- [ ] Test with various data types and edge cases\n- [ ] Verify tests work in different timezones\n- [ ] Test CI environment compatibility\n\n### Implementation Priority\nðŸš¨ **HIGH CRITICAL** - Blocking development workflow\n\n### Estimated Effort\n**2-4 hours** - Requires careful analysis and testing\n\n### Dependencies\n- ES Module configuration fix (previous issue)\n- Access to test environment\n\n### Notes for Copilot\n- Start with the simplest fix (date validation) to build confidence\n- Ensure all changes maintain existing API contracts\n- Add comprehensive test cases to prevent regression\n- Consider adding test utilities for date handling\n- Focus on making tests deterministic and reliable",
      "labels": ["type:bug", "priority:critical", "component:testing", "effort:medium", "status:ready"]
    }
  ],
  "high_priority_infrastructure": [
    {
      "title": "Implement production-ready Email Service with multiple providers",
      "body": "## ðŸ“§ Feature: Production Email Service Implementation\n\n### Current State\nEmail service is currently in simulation mode only:\n```javascript\n// Current implementation in server/services/email.js\nsetupEmailProvider() {\n  console.log('ðŸ“§ Email service initialized (simulation mode)');\n}\n```\n\n### Business Requirements\nThe Lightning Talk application needs reliable email functionality for:\n- Registration confirmations\n- Event reminders (7 days, 1 day before)\n- Speaker confirmations\n- Event cancellation notifications\n- Feedback requests\n\n### Technical Implementation\n\n#### 1. Provider Abstraction Layer\n```javascript\nclass EmailProvider {\n  async send(emailData) { throw new Error('Not implemented'); }\n  async verify() { throw new Error('Not implemented'); }\n}\n\nclass SMTPProvider extends EmailProvider { /* Implementation */ }\nclass SendGridProvider extends EmailProvider { /* Implementation */ }\nclass AWSESProvider extends EmailProvider { /* Implementation */ }\n```\n\n#### 2. Configuration Management\n```javascript\n// Environment variables needed\nEMAIL_PROVIDER=smtp|sendgrid|awsses\nSMTP_HOST=smtp.gmail.com\nSMTP_PORT=587\nSMTP_USER=user@domain.com\nSMTP_PASS=password\nSENDGRID_API_KEY=SG.xxx\nAWS_ACCESS_KEY_ID=xxx\nAWS_SECRET_ACCESS_KEY=xxx\nAWS_REGION=us-east-1\n```\n\n#### 3. Template System Enhancement\n```javascript\nclass EmailTemplateManager {\n  renderTemplate(templateName, data) {\n    // Support for HTML and text versions\n    // Variable substitution\n    // Internationalization support\n  }\n}\n```\n\n#### 4. Queue Management\n```javascript\nclass EmailQueue {\n  async enqueue(emailData, options = {}) {\n    // Add to queue with retry logic\n    // Support for scheduled sending\n    // Priority handling\n  }\n}\n```\n\n### Feature Requirements\n\n#### Core Features\n- [ ] **Multi-provider Support**: SMTP, SendGrid, AWS SES\n- [ ] **Template Management**: HTML/text email templates with variable substitution\n- [ ] **Queue System**: Reliable email queuing with retry logic\n- [ ] **Delivery Tracking**: Track sent emails and delivery status\n- [ ] **Error Handling**: Comprehensive error handling and logging\n- [ ] **Rate Limiting**: Respect provider rate limits\n\n#### Advanced Features\n- [ ] **Scheduled Emails**: Support for delayed sending (reminders)\n- [ ] **Batch Processing**: Efficient bulk email sending\n- [ ] **Template Validation**: Validate templates before sending\n- [ ] **Analytics**: Email open/click tracking (optional)\n- [ ] **Internationalization**: Multi-language email support\n\n### Implementation Plan\n\n#### Phase 1: Core Infrastructure (Week 1)\n1. [ ] Create provider abstraction layer\n2. [ ] Implement SMTP provider (most common)\n3. [ ] Update configuration management\n4. [ ] Add basic template rendering\n5. [ ] Write comprehensive tests\n\n#### Phase 2: Enhanced Features (Week 2)\n1. [ ] Add SendGrid and AWS SES providers\n2. [ ] Implement email queue system\n3. [ ] Add delivery tracking\n4. [ ] Implement scheduled sending\n\n#### Phase 3: Production Features (Week 3)\n1. [ ] Add monitoring and alerting\n2. [ ] Implement rate limiting\n3. [ ] Add email analytics\n4. [ ] Performance optimization\n\n### Acceptance Criteria\n\n#### Functional Requirements\n- [ ] Send registration confirmation emails automatically\n- [ ] Send event reminder emails at configured intervals\n- [ ] Handle email failures gracefully with retry logic\n- [ ] Support both HTML and plain text email formats\n- [ ] Validate email addresses before sending\n- [ ] Queue emails for reliable delivery\n\n#### Non-Functional Requirements\n- [ ] Handle 1000+ emails per hour without issues\n- [ ] 99.9% email delivery success rate\n- [ ] Maximum 5 second response time for email queuing\n- [ ] Comprehensive logging and monitoring\n- [ ] GDPR compliance for email data handling\n\n#### Security Requirements\n- [ ] Secure credential storage (environment variables)\n- [ ] Input sanitization for email content\n- [ ] Rate limiting to prevent abuse\n- [ ] Email content validation\n\n### Testing Strategy\n\n#### Unit Tests\n- [ ] Provider implementations\n- [ ] Template rendering\n- [ ] Queue management\n- [ ] Error handling scenarios\n\n#### Integration Tests\n- [ ] End-to-end email sending with real providers\n- [ ] Queue processing\n- [ ] Template rendering with real data\n- [ ] Configuration management\n\n#### Performance Tests\n- [ ] Bulk email sending\n- [ ] Queue processing under load\n- [ ] Memory usage monitoring\n\n### Configuration Example\n```javascript\n// config/email.js\nexport default {\n  provider: process.env.EMAIL_PROVIDER || 'smtp',\n  from: process.env.EMAIL_FROM || 'noreply@lightningtalk.com',\n  providers: {\n    smtp: {\n      host: process.env.SMTP_HOST,\n      port: process.env.SMTP_PORT,\n      auth: {\n        user: process.env.SMTP_USER,\n        pass: process.env.SMTP_PASS\n      }\n    },\n    sendgrid: {\n      apiKey: process.env.SENDGRID_API_KEY\n    }\n  },\n  queue: {\n    maxRetries: 3,\n    retryDelay: 5000\n  },\n  templates: {\n    path: './email-templates'\n  }\n};\n```\n\n### Implementation Priority\nâš¡ **HIGH** - Required for production deployment\n\n### Estimated Effort\n**2-3 weeks** - Comprehensive implementation with testing\n\n### Dependencies\n- Environment configuration setup\n- Email provider accounts (SendGrid, AWS)\n- Template design assets\n\n### Success Metrics\n- Email delivery success rate > 99%\n- Average queue processing time < 30 seconds\n- Zero critical email failures in production\n- User registration flow completion rate improvement\n\n### Notes for Copilot\n- Start with SMTP provider as it's most universally compatible\n- Implement comprehensive error handling from the beginning\n- Use existing email template structure as a starting point\n- Ensure all email content is properly escaped to prevent injection\n- Add detailed logging for debugging production issues\n- Consider implementing email preview functionality for testing",
      "labels": ["type:feature", "priority:high", "component:backend", "effort:large", "status:ready"]
    },
    {
      "title": "Upgrade Database Service from file-based to production-ready solution",
      "body": "## ðŸ—„ï¸ Feature: Production Database Implementation\n\n### Current State Analysis\nThe current database service uses file-based JSON storage:\n```javascript\n// Current implementation in server/services/database.js\nexport class DatabaseService extends EventEmitter {\n  constructor() {\n    this.dataDir = path.join(__dirname, '../data');\n    this.cache = new Map();\n    // File-based storage with in-memory caching\n  }\n}\n```\n\n### Business Justification\nFile-based storage limitations for production:\n- **Scalability**: Poor performance with large datasets\n- **Concurrency**: Risk of data corruption with concurrent writes\n- **Reliability**: No ACID transactions or backup/recovery\n- **Search**: Limited query capabilities\n- **Analytics**: Difficult to generate reports and insights\n\n### Technical Requirements\n\n#### Database Selection Criteria\n1. **PostgreSQL** (Recommended)\n   - âœ… ACID compliance\n   - âœ… JSON support for flexible schemas\n   - âœ… Full-text search capabilities\n   - âœ… Excellent Node.js ecosystem support\n   - âœ… Free and open source\n\n2. **Alternative: MongoDB**\n   - âœ… Document-based (similar to current JSON structure)\n   - âœ… Flexible schema evolution\n   - âœ… Good aggregation pipeline\n   - âš ï¸ Eventual consistency model\n\n### Database Schema Design\n\n#### Core Tables (PostgreSQL)\n```sql\n-- Events table\nCREATE TABLE events (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  title VARCHAR(255) NOT NULL,\n  description TEXT,\n  event_date TIMESTAMP WITH TIME ZONE NOT NULL,\n  location VARCHAR(255),\n  capacity INTEGER DEFAULT 50,\n  status VARCHAR(20) DEFAULT 'draft',\n  metadata JSONB,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n  updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Participants table\nCREATE TABLE participants (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  event_id UUID REFERENCES events(id) ON DELETE CASCADE,\n  name VARCHAR(255) NOT NULL,\n  email VARCHAR(255) NOT NULL,\n  participation_type VARCHAR(20) NOT NULL, -- 'onsite', 'online', 'undecided'\n  registration_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n  message TEXT,\n  newsletter_opt_in BOOLEAN DEFAULT false,\n  metadata JSONB\n);\n\n-- Talks table\nCREATE TABLE talks (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  event_id UUID REFERENCES events(id) ON DELETE CASCADE,\n  speaker_id UUID REFERENCES participants(id),\n  title VARCHAR(255) NOT NULL,\n  description TEXT,\n  duration INTEGER DEFAULT 300, -- seconds\n  status VARCHAR(20) DEFAULT 'proposed',\n  presentation_order INTEGER,\n  metadata JSONB,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Analytics events table\nCREATE TABLE analytics_events (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  event_type VARCHAR(50) NOT NULL,\n  entity_type VARCHAR(50), -- 'event', 'participant', 'talk'\n  entity_id UUID,\n  properties JSONB,\n  session_id VARCHAR(255),\n  user_agent TEXT,\n  ip_address INET,\n  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()\n);\n\n-- Indexes for performance\nCREATE INDEX idx_events_date ON events(event_date);\nCREATE INDEX idx_events_status ON events(status);\nCREATE INDEX idx_participants_event ON participants(event_id);\nCREATE INDEX idx_participants_email ON participants(email);\nCREATE INDEX idx_talks_event ON talks(event_id);\nCREATE INDEX idx_analytics_type_date ON analytics_events(event_type, created_at);\n```\n\n### Implementation Architecture\n\n#### 1. Database Connection Management\n```javascript\nimport pg from 'pg';\nimport { EventEmitter } from 'events';\n\nclass DatabaseService extends EventEmitter {\n  constructor(config) {\n    super();\n    this.pool = new pg.Pool(config);\n    this.initialized = false;\n  }\n\n  async initialize() {\n    await this.runMigrations();\n    await this.setupConnectionPool();\n    this.initialized = true;\n    this.emit('ready');\n  }\n}\n```\n\n#### 2. Migration System\n```javascript\nclass MigrationRunner {\n  async runMigrations() {\n    const migrations = await this.getMigrationFiles();\n    for (const migration of migrations) {\n      await this.runMigration(migration);\n    }\n  }\n}\n```\n\n#### 3. Repository Pattern\n```javascript\nclass EventRepository {\n  constructor(database) {\n    this.db = database;\n  }\n\n  async create(eventData) {\n    const query = `\n      INSERT INTO events (title, description, event_date, location, capacity)\n      VALUES ($1, $2, $3, $4, $5)\n      RETURNING *\n    `;\n    const result = await this.db.query(query, [eventData.title, /* ... */]);\n    return result.rows[0];\n  }\n\n  async findById(id) {\n    const query = 'SELECT * FROM events WHERE id = $1';\n    const result = await this.db.query(query, [id]);\n    return result.rows[0];\n  }\n\n  // Additional CRUD methods...\n}\n```\n\n### Migration Strategy\n\n#### Phase 1: Dual-Write Implementation (Week 1)\n1. [ ] Set up PostgreSQL database\n2. [ ] Implement new database service alongside existing one\n3. [ ] Write to both systems during transition\n4. [ ] Validate data consistency\n\n#### Phase 2: Data Migration (Week 2)\n1. [ ] Create migration scripts for existing JSON data\n2. [ ] Implement data validation and cleanup\n3. [ ] Perform bulk data migration\n4. [ ] Verify data integrity\n\n#### Phase 3: Service Switchover (Week 3)\n1. [ ] Update all database calls to use new service\n2. [ ] Remove old file-based system\n3. [ ] Monitor performance and stability\n4. [ ] Optimize queries and indexes\n\n### Feature Requirements\n\n#### Core Database Features\n- [ ] **Connection Pooling**: Efficient database connection management\n- [ ] **Transactions**: ACID transaction support for data integrity\n- [ ] **Migrations**: Version-controlled schema changes\n- [ ] **Query Builder**: Safe, parameterized query construction\n- [ ] **Connection Health**: Database health monitoring\n- [ ] **Backup Strategy**: Automated backup and recovery\n\n#### Advanced Features\n- [ ] **Read Replicas**: Scale read operations\n- [ ] **Connection Retry**: Automatic connection recovery\n- [ ] **Query Optimization**: Query performance monitoring\n- [ ] **Data Archiving**: Archive old event data\n- [ ] **Full-text Search**: Advanced search capabilities\n- [ ] **Analytics Queries**: Optimized reporting queries\n\n### Environment Configuration\n```bash\n# Database configuration\nDATABASE_URL=postgresql://user:password@localhost:5432/lightningtalk\nDATABASE_POOL_MIN=2\nDATABASE_POOL_MAX=10\nDATABASE_POOL_IDLE_TIMEOUT=30000\nDATABASE_QUERY_TIMEOUT=10000\n\n# Migration settings\nRUN_MIGRATIONS=true\nMIGRATION_TABLE=schema_migrations\n\n# Backup settings\nBACKUP_ENABLED=true\nBACKUP_SCHEDULE=\"0 2 * * *\"  # Daily at 2 AM\nBACKUP_RETENTION_DAYS=30\n```\n\n### Acceptance Criteria\n\n#### Functional Requirements\n- [ ] All existing functionality works with new database\n- [ ] Data integrity maintained during migration\n- [ ] Transaction support for critical operations\n- [ ] Backup and recovery procedures tested\n- [ ] Performance improves for large datasets\n\n#### Performance Requirements\n- [ ] Query response time < 100ms for simple operations\n- [ ] Support for 10,000+ participants per event\n- [ ] Handle 100+ concurrent database connections\n- [ ] Database startup time < 30 seconds\n\n#### Reliability Requirements\n- [ ] Zero data loss during migration\n- [ ] 99.9% database uptime\n- [ ] Automatic failure recovery\n- [ ] Complete backup/restore procedures\n\n### Testing Strategy\n\n#### Unit Tests\n- [ ] Repository pattern implementations\n- [ ] Migration scripts\n- [ ] Connection pool management\n- [ ] Transaction handling\n\n#### Integration Tests\n- [ ] End-to-end database operations\n- [ ] Migration testing with sample data\n- [ ] Performance testing with large datasets\n- [ ] Backup and recovery testing\n\n#### Load Tests\n- [ ] Concurrent user scenarios\n- [ ] Large event registration loads\n- [ ] Database connection limits\n- [ ] Query performance under load\n\n### Risk Mitigation\n\n#### Data Loss Prevention\n- [ ] Complete data backup before migration\n- [ ] Dual-write validation period\n- [ ] Rollback procedures documented and tested\n- [ ] Data integrity checks at each step\n\n#### Performance Monitoring\n- [ ] Database query performance monitoring\n- [ ] Connection pool utilization tracking\n- [ ] Slow query identification and optimization\n- [ ] Resource usage monitoring\n\n### Implementation Priority\nâš¡ **HIGH** - Critical for scalability and production reliability\n\n### Estimated Effort\n**3-4 weeks** - Complex migration requiring careful planning and testing\n\n### Dependencies\n- PostgreSQL server setup (development and production)\n- Database credentials and access\n- Data migration window coordination\n\n### Success Metrics\n- 10x improvement in query performance for large datasets\n- Zero data corruption incidents\n- 50% reduction in database-related errors\n- Support for 10x more concurrent users\n\n### Notes for Copilot\n- Start with a simple PostgreSQL setup for development\n- Implement comprehensive logging for debugging migration issues\n- Use parameterized queries exclusively to prevent SQL injection\n- Test migration scripts thoroughly with copies of production data\n- Implement gradual rollout strategy to minimize risk\n- Document all database schema changes for future reference\n- Consider implementing database seeding for development/testing",
      "labels": ["type:feature", "priority:high", "component:backend", "effort:large", "status:ready"]
    }
  ]
}